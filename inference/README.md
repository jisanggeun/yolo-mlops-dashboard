# Inference Server

## ğŸ“Œ Overview
YOLO ëª¨ë¸ì„ TensorRTë¡œ ìµœì í™”í•˜ì—¬ ì¶”ë¡ í•˜ëŠ” ì„œë²„ 
(Jetson Orin Nanoìš©)

## ğŸ“ Structure
```
inference/
â”œâ”€â”€ main.py         # FastAPI ì¶”ë¡  ì„œë²„
â”œâ”€â”€ convert.py      # YOLO -> TensorRT ë³€í™˜
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile      # Jetsonìš© Dockerfile
â””â”€â”€ README.md
```

## ğŸš€ Run (Jetson)
```bash
docker build -t yolo-inference .
docker run -d --runtime nvidia -p 8001:8001 yolo-inference
```

## ğŸ“¡ API Endpoints
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | /health | ì„œë²„ ìƒíƒœ í™•ì¸ |
| POST | /predict | ì´ë¯¸ì§€ ì¶”ë¡  |
| GET | /models | í˜„ì¬ ëª¨ë¸ ì •ë³´ |

## ğŸ”§ TensorRT ë³€í™˜
```bash
python convert.py best.pt
```